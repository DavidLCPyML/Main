# Overview of today: 
## What I did:  
Continued trying for that last couple of LB points. Got score to .689 with a higher resolution of 7200.
- Seems like that's the local maximum, will try to see if changing some training paradigms will help.
Last-minute fold training: trained models on 10/5 folds, using albumentations and without, tried to implement CLAHE/HSV
- not very much luck, best score was 0.591 on a model trained with 10 folds split sequentially.
      - ran out of GPU hours, so I will just have to wait until the contest ends.
- CLAHE I didn't see anything different, and HSV would keep throwing errors when I tried calling it as an augmentation.
Last minute model training - tried to see if I could get anything out of the models I made
- Tried implementing the stuff that the author of https://www.kaggle.com/c/tensorflow-great-barrier-reef/discussion/307045 mentioned.
      - Didn't see any improvements, perhaps because I tried too many things at once in a rush.
      - Tried the stuff he didn't say worked, more specifically GANs and using nonlabeled images as train data.
      - Unfortunately, couldn't figure out this one - I tried using multiple statements to thin out the dataframe of background images, but it would either include no background images or all of them.
- Combed through discussions and notebooks, trying to find anything I could experiment with for that final jump to 0.7+. 
      - Ultimately, couldn't find anything that I could implement quickly and accurately. Many techniques were high level-> just shows how much I have left to learn.
      - Wasn't paying attention to some models I was using and submitted a blank YOLO model. Needless to say that was a very poor accuracy.
- Had multiple instances where models would perform very well, only to underperform on the private set.
      - F2 evaluation would say they only missed <60 values with a CV of >0.8, only for them to score very low on the actual set.
      - Scores were 0.370, 0.472, 0.59, or something along that kind of line. 
      - Kind of frustrating, but nothing could be done anymore. GPU timne is out, so I will just finalize my model selection.
## What I now know:
- Sometimes things don't work out as intended.
      - That's ok, though.
      - I might have been too greedy with my training time - too much too late, I guess.
## Whatâ€™s next on the list:
- Pick models for final submit
## Data/Resources:
https://www.kaggle.com/c/tensorflow-great-barrier-reef/discussion/307045 - tips from high ranker about techniques that worked and didn't work for him
