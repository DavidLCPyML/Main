# Overview of today: 
## What I did:  
Picked final models. First contest, and I have around 70ish submissions, so I needed to be a bit smart with my cards here.
- 4 model submissions to finalize. All were done in Pytorch.
- Chose best CV model (0.675): with the CV-LB gap being as high as it was in the contest, I'll need this if the shakeup is the way I expect it to be.
      - Some of the most recent contests (Jigsaw Toxicity) had a huge shakeup from people only focusing LB.
      - This one also has a lower res than the other, so it won't suffer from a huge distributional shift.
- Chose best LB model (0.689): Chosen just in case the shakeup is not as severe as previous contests.
      - Other rationale: The previous contest with that huge shakeup was an NLP contest. 
      - Since this is CV, which is usually more consistent with its factors, this shouldn't have too much of a shakeup, right?
- Chose best ensemble model (0.632): Chosen just in case ensembles actually end up surviving the shakeup.
      - Ensembles are already really common, so I'm not sure why they don't do well on public LB. Maybe the opposite would be the case on the private set.
      - I think most of the top scorers used some sort of variable length model ensemble, and I'm pretty sure some said they saw a small improvement in CV/LB.
- Chose a random model (0.593): Honestly, this one was probably a pride thing, since this was the highest scoring model I wrote completely by myself.
      - Though then again, I'm kind of happy I managed to score almost 0.6 in my first ever CV contest. 
      - Though my submissions were legit, I feel like my model isn't since I had to use a publically released notebook. 
      - I'd like to see what would have been if I had just used my own things from the ground up.
- I'd have liked to implement some kind of postprocessing though. 
      - Like maybe a model to predict areas of interest, then a finetune to better focus on areas of interest?
      - If I could, say, process the predictions so that less confident predictions between a series of high confidence predictions would be normalized to the "lowest highest" confidence, I might be able to avoid FNs?
      - I'd have also liked to see SWIN or DETR at play here. Shame they did so poorly on LB. Cascade-RNN seems to be doing freakishly well, though (according to 2nd place, they are using a Cascade-RNN submission).
## Whatâ€™s next on the list:
- Wait until contest ends, then write results, I guess.
