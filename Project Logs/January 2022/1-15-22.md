# Overview of today: 
## What I did:  
Ran said high-res train notebook & compared against my model. As exected, it does do a lot better (0.492 compared to 0.362), increase of 0.13!  
Saved a copy of the author's model weights as a reference, could be used with my models in a promising ensemble.
- So it does seem like it works. Kinda out there, could it be that the original size was that?
- Tried to replicate results. Seemed to work correctly, F2 was really close to the baseline (0.576) after a few modifications.
    - Heavier mixup value used (0.7), lower fliplr (0.3), image res lowered to 3000 and batch size to 4, train with 10 epochs.
    - As an aside, I tried using adjusting the "degrees", "shear", and "translate" params, but doesn't seem like it help with anything. At least, the params need to be larger.
    - Total run time was 5 hours, approximate best F2 score at end of run was around 0.5.
    - Precision around the 0.7 range, recall around the 0.5 range. Still need to work on bolstering that recall score.
- Experimented with larger model, yolov5m6. had a slightly higher F2, but due to the size I had to cut down on the image res (2000-8), so I'm unsure if it's worth it.
    - Needs more tweaking. Maybe more augs will help, like with yolov5s6. 
Experimented with the infer notebook, some interesting results found.
- size=3600 had .579, size=9000 had 0.613, size=10000 had 0.612. size=12600 sent us back to below the starting baseline (0.557).
    - I sent in a size=14400, but since size=12600 took ~7,8 hours to run, chances are this will time out.
    - It seems 3x off native and 3x off train seems like an ideal split, confirming the original author's ideas. I still don't get why exactly that's the case.
- Jumped to the upper 200s with this kernel submission - promising results, but I still don't feel too certain about the tactics used (infer @ 9x of dataset res).
    - Perhaps 1.5x is a better estimate. 3x seems too outlandish, it'd be too blurry to see anything.
- Out of submissions for today, will consider tuning model CONF and prediction CONF. Doesn't seem like I can find the IOU variable, so that will have to wait until tomorrow.
    - The experiments to make: lower pred conf, higher pred conf, higher model conf. Maybe increments of 0.05? 
YOLO-Z:
- Still trying to map everything together. FPN implementation is coming along smoothly, but the XS fMap is still giving me trouble. 
    - It's something about the prediction layer - I can't find which layer handles it correctly.
## What I now know:
- Increasing image resolution doesn't help with features or anything. It just makes the image larger, and a bit blurrier. So larger objects suffer more than smaller ones.
    - So perhaps this is inferring at the inherently native resolution. Perhaps, the dataset's original size was 3600? 
    - Images of that size would OOM(out-of-memory) for Kaggle GPUs, so maybe they scaled things down to make it fair for GPU-less people? 
- A lower model confidence handles whether or not to show a bounding box in predictions. 
    - Probabilities are picked out beforehand, so it's just a matter of filtering out the low confidence boxes.
## Whatâ€™s next on the list:
- Read up on source code for YOLO.
    - Try a larger model. Maybe I'd get better results with a scaled up model (more features/boxes to use?)
    - Determine how to split and cross-validate. (more specifically, by sequence, video, or some other metric)
    - Implement any augmentation techniques. (any custom/predefined augmentations I can use to improve P/R in training?)
    - See if I can implement the norfair tracker in my models. Haven't tried it, but it could help.
    - Test-time augmentation may help as well. Let's give it a shot. (put simply, any postprocessing I can do?)
- Continue trying to finish YOLO-Z (small), especially with the XS feature map. If not able to, just using this as is works for me as well.
## Interesting tidbits/thoughts:
- Looks like there'll be a big jump. I wonder if I can stay in front of the curve?
## Data/Resources
N/A
