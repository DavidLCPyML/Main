# Overview of today: 
## What I did (past 2 days):
Worked on trying to make a YOLO-Z training notebook. Eventually gave up, as I couldn't for the love of me figure out how to implement a DenseNet(In this case, a DenseNet-50).
- I can't figure out how to swap out the layers of the backbone without breaking everything, and I have no clue how to decrease the layers in a normal DenseNet without also breaking everything.
- Techniques attempted: 
    - Modifying the yaml containing the layers (broke the train.py's interpreter, some kinda weird traceback error)
    - Rebuilding by rewriting the layers in a different yaml and loading that (failed due to layer mismatch)
    - Tried to rewrite code for layers (done out of desperation, failed due to lack of knowledge about structuring)
    - Tried to use other yamls in the models hub (successful, but no noticeable improvement in LB).
- DenseNet's got 200+ layers in it, but all the layers in it look equally important to me, so downsizing it to a 50-layer network is tough without horribly destroying my accuracy.
    - Also, they mention something about ResNet, so I'm kinda confused as to what exactly I'm supposed to use.
- Unfortunately, this idea is now scrapped, I can't figure this out. It's a shame, but I don't think I'll have it up in time properly. *sigh*
## Whatâ€™s next on the list:
- Read up on source code for YOLO.
    - Determine how to split and cross-validate. (more specifically, by sequence, video, or some other metric)
    - Implement any augmentation techniques. (any custom/predefined augmentations I can use to improve P/R in training?)
    - Check if temporal tracking can help.  (multi- or single object temporal tracking?)
    - Test-time augmentations/preprocessing may help as well (put simply, any postprocessing I can do?)
## Interesting tidbits/thoughts:
- No thoughts here, other than my frustration about my failure to implement YOLO-Z. A good lesson learned, though - Don't bite off more than you can chew.
