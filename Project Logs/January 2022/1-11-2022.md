# Overview of today: 
## What I did:  
Submitted and evaluated first YOLO prediction. Needless to say, results were kinda... tragic, to say the least.  
An F2 LB score of 0.005, that is kinda upsetting. I'm probably doing something weird with my train set, since there's no way YOLOv5 does that terribly.  
Even BERT/SWIN do at least 0.3!
Theories, at least train-wise: 
- I am failing to detect any COTS - that is, I am not training properly and the model is trying to look for stuff that doesn't exist.
    - So, either I am only training on background pics, or my FP rate far outstrips my TP rate (like by 100x), throttling my F2.
    - Too many FN is also a problem: F2 weights recall more, so a FN will kill my score.
- I have way too much data leakage - the CV split is not hiding enough data to generate a strong, generalizable model.
    - Maybe if I split by video (3 folds) instead of a naive 80-20 split, I'd get better results? 
    - I could hide video 2, since it has 650+ frames, about 1/7th of the dataset, approximately equal to an 80-20 split. Training will just take a bit longer.
    - Oh yeah, I should check if I split the videos randomly, or if I did them in chronological order. This is a video, after all.
- Train augmentations are too heavily augmented to even remotely resemble the dataset. Highly unlikely, as I am using default hyperparameters.
    - Maybe the learning rate is too high? Adam does learn faster than SGD...Maybe I'm converging too fast/overshooting the extrema?
- Training for too long, or training at too small of an image resolution
    - Too long = overfit, too small res = information compressed->loses features?
    - Perhaps F1 is too different from F2, so recall is too poor. 
- Underfitting is also possible - there's so few dimensions that the model can predict, and suddenly being given data of higher dimensions confuses it and causes garbage predictions.
Test time:
- I am predicting the classes properly, but the boxes are all encompassing background data. 
    - AKA too many false positives on "fake" starfish, or the box encompassing the "predicted" starfish has too low of an overlap with the ground truth(GT) box.
    - So in short, either my FP:TP is 100:1, my IOU(intersection-over-union) with the GT is too small, or a mix of both.
- I loaded in the wrong model. Kinda anticlimactic, but that's the only thing I can think went wrong.
**Found a model called YOLO-Z which is excellent for small object detection in real time. Almost exactly what I needed!**
- I'll try to see if I can implement this, but if I have to modify each layer one by one, I'll probably skip for now as it's out of the scope of my knowledge.
- Though, my main concern is trying to troubleshoot the terrible score on this model first. 
## What I now know:
- Train predictions don't always match up with test-time data. Also, boxes are hard. Like, crazy hard.
- For temporally aligned data, CV should encompass contiguous frames. Otherwise, data leakage from CV will happen, causing the valid set to have less worth.
- A variety of factors can cause CompVision models to perform poorly. This is true for almost all models. 
## Whatâ€™s next on the list:
- **Fix this model, or scrap it altogether**. Prefereably, graphs showing the validation and GT predictions would be useful.
- Read up on source code for YOLO.
    - See if training parameters or even the model architectures themselves can be tweaked.
    - Determine how to split and cross-validate.
    - Try to see if I can view the predictions in image format. 
    - On train images, show the actual bounding boxes overlaid onto the train image, and compare what's going on with GT.
- See if I can implement a proper/similar YOLO-Z model.
    - If successful, compare results with a proper YOLO result. How will it look? Will it generalize?
## Interesting tidbits/thoughts:
- The most probable factor I can think of here is my CV leakage. 
    - Too many items are leaking into the train, and the model's extrapolating predictions based on train preds.
- The other factor I can think of is really high precision, but really low recall. 
    - The IOU is too low for each prediction, or something is causing recall to be terrible on test time.
## Data/Resources
- https://machinelearningmastery.com/data-preparation-without-data-leakage/: I'll probably use this to fix any data leakage. Maybe that'll help.
