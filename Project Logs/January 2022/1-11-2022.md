# Overview of today: 
## What I did:  
Submitted and evaluated first YOLO prediction. Needless to say, results were kinda... tragic, to say the least.  
An F2 LB score of 0.005, (1400/1500, ouch!) that is kinda upsetting. I'm probably doing something weird with my train set, since there's no way YOLOv5 does that terribly.  
Even BERT/SWIN do at least 0.3!
Theories, at least train-wise: 
- I am failing to detect any COTS - that is, I am not training properly and the model is trying to look for stuff that doesn't exist.
    - So, either I am only training on background pics, or my FP rate far outstrips my TP rate (like by 100x), throttling my F2.
    - Too many FN is also a problem: F2 weights recall more, so a FN will kill my score.
- I have way too much data leakage - the CV split is not hiding enough data to generate a strong, generalizable model.
    - Maybe if I split by video (3 folds) instead of a naive 80-20 split, I'd get better results? 
    - I could hide video 2, since it has 650+ frames, about 1/7th of the dataset, approximately equal to an 80-20 split. Training will just take a bit longer.
    - Oh yeah, I should check if I split the videos randomly, or if I did them in chronological order. This is a video, after all.
- Train augmentations are too heavily augmented to even remotely resemble the dataset. Highly unlikely, as I am using default hyperparameters.
    - Maybe the learning rate is too high? Adam does learn faster than SGD...Maybe I'm converging too fast/overshooting the extrema?
- Training for too long, or training at too small of an image resolution
    - Too long = overfit, too small res = information compressed->loses features?
    - Perhaps F1 is too different from F2, so recall is too poor. 
- Underfitting is also possible - there's so few dimensions that the model can predict, and suddenly being given data of higher dimensions confuses it and causes garbage predictions.
Test time:
- I am predicting the classes properly, but the boxes are all encompassing background data. 
    - AKA too many false positives on "fake" starfish, or the box encompassing the "predicted" starfish has too low of an overlap with the ground truth(GT) box.
    - So in short, either my FP:TP is 100:1, my IOU(intersection-over-union) with the GT is too small, or a mix of both.
- I loaded in the wrong model. Kinda anticlimactic, but that's the only thing I can think went wrong.
**Found a model called YOLO-Z which is excellent for small object detection in real time. Almost exactly what I needed!**
- I'll try to see if I can implement this, but if I have to modify each layer one by one, I'll probably skip for now as it's out of the scope of my knowledge.
- Though, my main concern is trying to troubleshoot the terrible score on this model first. 
## What I now know:
- Train predictions don't always match up with test-time data. Also, boxes are hard. Like, crazy hard.
- For temporally aligned data, CV should encompass contiguous frames. Otherwise, data leakage from CV will happen, causing the valid set to have less worth.
- A variety of factors can cause CompVision models to perform poorly. This is true for almost all models. 
## Whatâ€™s next on the list:
- **Fix this model, or scrap it altogether**. Prefereably, graphs showing the validation and GT predictions would be useful.
- Read up on source code for YOLO.
    - See if training parameters or even the model architectures themselves can be tweaked.
    - Determine how to split and cross-validate.
    - Try to see if I can view the predictions in image format. 
    - On train images, show the actual bounding boxes overlaid onto the train image, and compare what's going on with GT.
- See if I can implement a proper/similar YOLO-Z model.
    - If successful, compare results with a proper YOLO result. How will it look? Will it generalize?
## Interesting tidbits/thoughts:
- The most probable factor I can think of here is my CV leakage. 
    - Too many items are leaking into the train, and the model's extrapolating predictions based on train preds.
- The other factor I can think of is really high precision, but really low recall. 
    - The IOU is too low for each prediction, or something is causing recall to be terrible on test time.
## Data/Resources
- https://machinelearningmastery.com/data-preparation-without-data-leakage/: I'll probably use this to fix any data leakage. Maybe that'll help.
