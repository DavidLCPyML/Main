# Overview of today: 
## What I did:  
Implemented the 2D real-time object tracker Norfair into my inferencing pipeline. 
- Worked pretty well - LB jumped from .627 to 0.642, which is pretty nice. 
    - The best LB did poorly with tracking, likely because I was overfitting to the public LB.
- I did have to keep TTA on for the submission, though - it seems that TTA off hurts its predictions, likely because of all the false positives being triggered by background images.
    - Though honestly I have no idea why is this - I'll need to read up.
Explored some other test-time augmentation techniques, such as ensembling, inpainting, and SAHI (Slicing-Aided Hyper Inference). Some worked, others didn't.
- Awfully suspicious about SAHI: it works freakishly well on CV, but public LB punishes it hard (almost a 9% drop compared to baseline).
- Ensembles also seem to suffer from the same fate as norfair, albeit worse with a drastically increased FP rate hurting accuracy. 
    - Ensembles on my end do okay - comparatively worse than best model, but still fairly decent (bit lower than best model).
    - However, maybe different architectures could help its inferencing by adding the extra few TPs. I might be facing a drop because all my models are YOLO.
    - I could try a RCNN-based network mixed with a YOLO to help increase accuracy - the author claimed they had different learning paradigms, so an inference could work really well.
- Perhaps I could do some manual preprocessing? Like if a prediction only shows up for a couple of frames, could I manually code a method that "changes its mind"?
- Inpainting seems fun and novel, but I don't think it'll help me.
    - The main premise is that by placing false COTS in background areas of the train image the model will be better able to identify the features of COTS.
    - However, personally I don't think I'll see much improvement, as there may be fuzzy areas where it didn't fully integrate. 
    - The model may exploit this and become reliant on trying to find mismatched pixels, which means a poor generalization.
    - The other problem is consistent time frames for each COTS. if they stay stationary throughout the video that would be too strange-looking.
## What I now know:
- There must be some sort of mismatch between train and public LB. 
    - Chances are, the public LB has smaller boxes or consists of mostly smaller objects, as larger res helps out, but general-purpose object inference aids seem to work poorly.
    - Also, the fact that everything underwater is overssaturated with blue may be a problem. 
    - Perhaps I could try normalizing the images to more closely mimic what it looks like above water?
## Whatâ€™s next on the list:
- Read up on source code for YOLO.
    - Determine how to split and cross-validate. (more specifically, by sequence, video, or some other metric)
    - Implement any augmentation techniques. (any custom/predefined augmentations I can use to improve P/R in training?)
## Interesting tidbits/thoughts:
- Big jump again, no longer in top range. Oh, how the mighty have fallen.
- Both a shame and a mystery as to why SAHI doesn't work. Perhaps it misses too many objects? 
    - FP rate seems pretty good, so the only thing I can think of is objects getting missed by being cut exactly on the slices.
    - But even then, that shouldn't be the case, as the documentation has measures to aid with combining partially predicted objects...
- Looks like I picked a bad time to make my LB jump, haha.
## Sources:
- https://www.kaggle.com/remekkinas/sahi-slicing-aided-hyper-inference-yv5-and-yx: SAHI TF notebook I borrowed code from
- https://www.kaggle.com/parapapapam/yolox-inference-tracking-on-cots-lb-0-539: Norfair tracker implementation in YOLOX - copied code & modified to suit YOLO
- https://www.kaggle.com/kocha1/only-yolov5-tracking-lb-642: YOLOv5 tracker implementation someone else released today
