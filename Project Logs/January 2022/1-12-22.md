# Overview of today: 
## What I did:  
Retweaked the model. It should work properly now, as I ensured there is no noticeable validation leakage anymore.
- It was most definitely a validation leak. Almost 70-80% F1 for the frame sequences that leaked, and 10-20% for the unleaked sequences.
    - Further investigation showed that the 80-20 CV fold split video 1 into 2 parts, causing data leakage into CV, which the model abused using its trained V1 data, yielding false metrics.
    - Just did a split by video, it's easier and simpler too. Each vid is in a different location, so there should be no CV data leakage.
- Oh yeah, I was also ordering each image in the video randomly during training. That played a HUGE factor in why I did terribly. Video frames are supposed to be chronological, you dummy! 
    - I don't even know where that even came from. What kind of idea was that??
- No custom augmentations, everything is default hyperparams. I want to see how my pure baseline does before I bother with higher level augmentations.
- Able to see validation pred/GT labels now.
    - I think I fixed the metric scorings, but I'll have to wait till tomorrow.
- Funny thing is, according to the corrected valdata some COTS are predicted in 1 frame, but aren't in the next chronological frame. 
    - If I can implement temporal consideration of previously predicted objects, I could get more consistent results
    - Since there's no way starfish disappear in the center of a frame (they move too slowly from frame to frame), I could possibly add a consideration during test time?
Read up on YOLO-Z. No code implementations, neither PyTorch or TF, so there will be some modifying needed.
- I think I know enough about YOLOv5 to implement the 3-anchors, wide structure, and FPN head, but I'm not sure about DenseNet and the XS inclusive feature map.
    - I could use the fpn yaml in the lib, anchors is tunable in the yaml, wide should just be using the m/m6 structure. too limited to use l6.
    - DenseNet will be a problem - I don't know how that would fit. Also, XS inclusive is also troublesome, I don't know nearly enough about CV to understand what happens here.
- How fast would this be train-wise? I'm kinda concerned about that...but, the payoffs seem so good! Almost 3-6% improvement in mAP!
## What I now know:
-  YOLO consists of pre-build yaml files that tell the parser what type of model to compile. If modified properly, one can create a derivative of YOLO that is better suited to the task.
-  I can mix & match different parts, like the head, neck, and backbone, so long as they mesh correctly.
-  Changing the metric implementation can be found in train.py, val.py, metric.py, init.py(in utils base), and more. 
    -  Tried getting YOLO to train off of F2 score, but no dice here. (kept throwing error on improper spacing before training?) Looks Like I'll have to optimize both metrics to be safe.
## Whatâ€™s next on the list:
- Read up on source code for YOLO.
    - Determine how to split and cross-validate.
    - Implement any augmentation techniques.
    - Check if temporal tracking can help.
    - Test-time augmentation may help as well. Let's give it a shot.
- See if I can implement a proper/similar YOLO-Z model.
    - If successful, compare results with a proper YOLO result. How will it look? Will it generalize?
## Interesting tidbits/thoughts:
- YOLO-Z's XS inclusive map is a toughie. I'll have to find out how to create another prediction layer without breaking the thing entirely.
    - Should be on the 1st convNet as well. Let's try our best.
- F1 score is pretty high, but recall is kinda bad. I hope to remedy this problem with F2 indicators.
## Data/Resources
https://arxiv.org/pdf/2112.11798.pdf: YOLO-Z paper
