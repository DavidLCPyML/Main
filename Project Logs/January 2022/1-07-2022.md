# Overview of today: 
## What I did:  
Began writing my code in a Kaggle notebook. Kinda shooting in the dark here, I might be going too fast. 
- Began with taking in data & splitting it.
- Decided to not include background images, AKA only train on data with COTS objects in them. 
    - around 4919 frames with at least 1 COTS object. The rest are background images with no objects. 
- Let's do a naive 80-20 split. Upper 80% of frames are trained, the remaining half are validated, probably via group K-fold.
- If possible, I want to do 5+ folds. With only 3 videos, that may be a problem, especially when they're not all the same length. More experiments needed.
    - Need to clean the data as well: Restrict only to COTS and properly map each truncated train entry to its proper image, otherwise I don't think this will work.
    - Need to copy labels to separate directory. Probably not necessary, but we will see.
- I'll first try it with YOLO(v5). Train at 1280, as high of a batch size allowable, and 10-ish epochs?
    - needs to define hyperparameters/augmentations, train/test location, and dataframe of COTS annotations.
- Wrote functions to convert from COCO to YOLO format. YOLO's predictions will be wrong otherwise. 
    - This was the hard part, as I needed to calculate a lot of things, as well as write code to trim any bboxes that overlapped with any image frame edges.
- Need to set up wandb account!
    - I gotta see my metrics!
## What I now know:
- I think I understand the general hang of the dataset. Seems to me like the hard part'll be performing modifications to model architecture or the inference augmentations. 
- YOLO (at least, YOLOv5) allows for implementations of augmentations during training, which doesn't necessarily have to be written from scratch.
    - Not really sure what each augmentation does, but I'm sure I'll find out as the comp goes on.
    - Actual training doesn't seem to terrible either: loading it is a couple of lines, and actual training is also a 1-2 command program too. 
- How to convert COCO boxes to other metric implementations, namely YOLO/Pascal-VOC format.
- How to load and train a YOLOv5 model on custom data. We only have one class, so no need for the 80+ VOC/COCO classes that their YOLO has. I'll still use the pretrained wieghts, though.
## Whatâ€™s next on the list:
- Read up on code documentation for the models mentioned above (SWIN, DETR, etc.).
    - See if training parameters or even the model architectures themselves can be tweaked.
    - Determine how and what to cross-validate.
    - Figure out how to port finished models for inferencing. I don't want to have to retrain each time I submit, since that would be too tragic and a waste of my GPU time.
- Find ways to plot progress on a separate location that I can access later. Probably with WandB. 
## Interesting tidbits/thoughts:
- YOLO is 200+ layers! I thought Darknet-53 was a lot, but that's just the backbone! That's almost 40x the size of my largest model!
- I wonder how much space YOLO takes - my Captcha net only took up like 1-2 gigs of ram max. Hopefully it doesn't scale with layer size, since I'd be going nowhere.
- Could I ensemble DETR and YOLO? or would an RCNN model be better? I think on the original YOLO paper said that RCNN and YOLO ensembles did really well. 
## Data/Resources
- https://github.com/ultralytics/yolov5: The big man Glenn Jocher's implementation of YOLO, complete with tutorials and graphs on performance
