# Overview of today: 
## What I did:  
Finished up the kernel. Doesn't look too good, there are probably a lot of messy things, but it gets the job done. 
- Full training pipeline of YOLOv5. haven't run it in its entirety but one by one the cells all work correctly.
- Got the wandb account set up as well as proper linkage (I think?) 
    - It seems to work properly, after all. 
- Reviewed public top scoring Kaggle notebooks - looks like most notebooks use something called YOLOX, a derivative of YOLOv5 claimed to be better than it.
    - Scores here seem to be just above the 0.5 range (looks like most are at 0.5). YOLOv5 is also a biggie, with scores in the same range.
    - MMDet transformer models seem to be a lot worse, being at the 0.4s range. Maybe this isn't their cup of tea?
- Copied over some code from this cool notebook: https://www.kaggle.com/awsaf49/great-barrier-reef-yolov5-train
    - mostly used the code to clean up certain areas, such as outsourcing the "cocotoYOLO" suite of functions, as well as using the code for label transformations.
    - Also copied over the code to initialize the label and train directories. Code is a lot cleaner now.
    - Some nice graphs were also made in the notebook, so I borrowed code from there, too. More graphs is always good, especially when you're dealing with a large dataset.
## What I now know:
- How to train and load a model from GitHub, as well as how to save said model's weights.
- How to use W&B to log my metrics
- How the various augmentations work during training time(at least from the comments and a general google search of the hyps)
## Whatâ€™s next on the list:
- Read up on code documentation for the models mentioned above (SWIN, DETR, etc.).
    - See if training parameters or even the model architectures themselves can be tweaked.
    - Determine how to split and cross-validate.
    - Figure out how to port finished models for inferencing.
- See if I can plot anything else that might be useful. Maybe the F1 curve, or the P/R/PR curves? Don't think mAP is useful in this context. 
## Interesting tidbits/thoughts:
- YOLO takes up a lotta memory. dimensions 1280, batch size 16 is the max here for a 16GB Kaggle GPU.
- Funny that Swin and DETR did so poorly, considering that their scores are comparable with YOLO on the COCO/VOC set as well as other benchmarks.
## Data/Resources
- https://www.kaggle.com/awsaf49/great-barrier-reef-yolov5-train: the aforementioned notebook that I borrowed graphing+training code from
