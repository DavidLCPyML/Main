# Overview of today: 
## What I did:  
Model submitted & evaluated today. LB F2 Score: 0.362, jumped to around the 1000s. Comparatively bad compared to the baseline, but at least I have a decent "proof of concept" baseline.  
New notebook published today that scores higher than the 0.539 YOLOX baseline, discussing something about a higher resolution on YOLOv5. Here are the insights that popped up.
- Uses YOLOv5. I guess I was on the right track here about YOLO being a strong contender to win.
    - Hardware used is an RTX 3090TI (32GB VRAM). Hard to replicate with my Kaggle P100 (16GB VRAM), I'll be inherently worse with an even lower batch size, but no time to complain.
- Changed up some of the hyperparams, most notably the "mixup" and "fliplr" params. Edited learning rate to be slower, especially since now I decided to use Adam instead of SGD.
    - Mixup probably involves something with a copy-paste combo of images, while fliplr probably governs the chance of flipping an image.
    - lr -> 0.001, was previously 0.01 with SGD.
- Supposedly claims that training at 3x of the dataset's resolution can cause better model generalization. (train notebook)
- Inferring at 3x that original resolution also helps improve LB. That's weird, you wouldn't be getting any information from that?? (infer notebook)
    - Well, whatever works, I guess. I'll have to investigate on my own time.
- Confidence is at 0.01 - a very strange value. That jacks up your FP rate, at the cost of only a couple of TPs. Is it worth it?
Working on a YOLO-Z pipeline. Data and everything else seems almost identical, since they are built off each other.
- Anchors and width have been adjusted, but I'm having trouble with the feature map and FPN integration (file exists, but not in the same versions, s-fpn.yaml vs s6.yaml).
## What I now know:
- Augmentations can help influence accuracy. Perhaps others could as well?
- Lower model confidence can help squeeze out a couple extra TPs, but at the cost of FP rate increasing. Same thing with modifying the comparison IOUs.
    - Have to be careful, though - too low of a conf/IOU threshold leads to garbage output as every bbox predictor suddenly becomes relevant.
## Whatâ€™s next on the list:
- Read up on source code for YOLO.
    - Try a larger model. Maybe I'd get better results with a scaled up model (more features/boxes to use?)
    - Determine how to split and cross-validate. (more specifically, by sequence, video, or some other metric)
    - Implement any augmentation techniques. (any custom/predefined augmentations I can use to improve P/R in training?)
    - Check if temporal tracking can help.  (multi- or single object temporal tracking?)
    - Test-time augmentation may help as well. Let's give it a shot. (put simply, any postprocessing I can do?)
- See if I can implement a proper/similar YOLO-Z model.
    - If successful, compare results with a proper YOLO result. How will it look? Will it generalize?
## Interesting tidbits/thoughts:
- I wonder if training at an even higher resolution does anything. Would that even be effective if I trained at sizes of 4800 and higher?
    - Moreover, will the models even run in reasonable time if the images are blown that high? 
    - It sounds so crazy I'd instantly shoot it down if not for the fact that it improves the public baseline to 0.579.
- Looks like most multi-object trackers do poorly here, only norfair seems to have a positive affect on LB.
## Data/Resources
- https://www.kaggle.com/steamedsheep/yolov5-high-resolution-training: reference train notebook
- https://www.kaggle.com/steamedsheep/yolov5-is-all-you-need: reference inference notebook
