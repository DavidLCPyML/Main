# Overview of today: 
## What I did:  
Experimented with the infer notebook, some interesting results found, this time on model confidence.
- Increasing the row conf to a certain threshold increased the score. I hit 0.627! Now I'm in the top 50s! Crazy!
    - Confs @ img size 9000: 0.15-0.613, 0.2-0.623, 0.25-0.627, 0.1-0.597, 0.3-0.626.
    - Seems the optimal conf for eliminating garbage FPs is around 0.25. 
    - Maybe the true maximum is between the ranges (0.25, 0.3) and (9000,10000), but that'd definitely be overfitting, which I don't want.
Wasted a couple of submissions that fell through - Didn't use GPU accelerators, so the inference time spiked and caused the notebook to time out.
    - GPU is good for image inferencing, who would've figured? (it's a joke, of course it is, it's responsible for rendering images quickly!)
Twiddled around with model params - kept trying to see if they did anything to the P/R curves. Not much of an effect, at least from a glance.
- YOLOv5m6 was also okay, but didn't see much improvement compared to the time and space needed for it, so I just decided to use s6.
- P/R for m6: 0.7/0.5, P/R for s6: 0.8/0.45. Not much of an F2 improvement, LB only changed by about 0.001.
- Played around with visualizing pred/GT videos - tried to visualize the frame-by-frame predictions. Lots of variation in detections.
    - Huh. This model's kind of funky - it can pick up COTS, but in many cases it's like a one-off-prediction.
    - COTS are almost always detected in AT LEAST 1 frame of the entire sequence they're in. A solution to this could be norfair tracking, if I can get it to work.
    - There was a couple COTS that straight up were missed by the model. That needs to change, maybe train augmentation could work.
## What I now know:
- Increasing model confidence doesn't seem to do much. At least, doing a postprocessing on prediction confidence removes any model annotation below a certain confidence threshold.
    - So this is some sort of postprocessing. Maybe I could scale this up when I add other test-time inference aids?
- Larger models will perform better, but at the cost of increased inference time. Though, with a 16GB RAM I can't see much of a difference either way.
- A tracker allows the model to recognize to some extent the temporal dimension, allowing for predictions to be much smoother and fleshed out if multiple are detected in a sequence.
## Whatâ€™s next on the list:
- Read up on source code for YOLO.
    - Try a larger model. Maybe I'd get better results with a scaled up model (more features/boxes to use?)
    - Determine how to split and cross-validate. (more specifically, by sequence, video, or some other metric)
    - Implement any augmentation techniques. (any custom/predefined augmentations I can use to improve P/R in training?)
    - Check if temporal tracking can help.  (multi- or single object temporal tracking?)
    - Test-time augmentation may help as well (put simply, any postprocessing I can do?)
- See if I can implement a proper/similar YOLO-Z model.
    - If successful, compare results with a proper YOLO result. How will it look? Will it generalize?
## Interesting tidbits/thoughts:
- Yeah, this is definitely not scalable. I'll need a better way to conduct and verify experiments.
    - Possible augmentations to the training process: Higher res (probably not sustainable), heavy rotations and translations, mosaics, and probably color modifications.
    - My main concern is that this will be a GPU race. Higher res training could just be the trick, and people with more RAM can just go ham, which is bad for me. :(
    - Maybe I could use a color augmentor that gets rid of the blue. The underwater images are all blue, so maybe I could get at least some bbox predictors to identify shape of the starfish better?
- I haven't tried GAN techniques for dataset bolstering, nor any models from the MMDET libraries. Could they add any improvements?
- Transfer learning may work too - I could train at a large res and then scale down after freezing the backbone, kinda like a general->specific feature identifier.
