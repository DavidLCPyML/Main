# Overview: 
## What I did (over the past month or so):  
- Brushed up on fundamentals needed to understand the libraries used, such as vector mathematics, derivatives(specifically gradients), probability distributions, etc.
    -  source: http://themlbook.com/wiki/doku.php
-  Took notes & learned about machine learning techniques and the math behind them
## What I now know:
-  Quite a lot about the fundamentals of Supervised and Unsupervised learning
    -  Most of the methods used are not "100%" autonomous since they require tuning of **hyperparameters** that the data analyst needs to do beforehand.
    -  Feature engineering and normalization is actually really important.
-  Many ML problems are **efficiently solvable with sklearn** and other library functions, so usually there is no need to make custom functions.
    -  The most basic functions are log-linear Regression, Decision Trees, K-Nearest Neighbors, and Support Vector Machines(SVM). Many later methods build off these simple algorithms. 
    -  **Mean Squared Error** is a pretty common loss function due to its **easy differentiability** (linear), especially in regression. 
    -  Other basic functions include **sigmoid and softmax(generalization of sigmoid) functions** for logistic prediction.
-  Neural networks like to use logistic regression a lot - it consists of many nested functions in an attempt to generalize relationships found in data.
    -  The output layer is usually a softmax or ReLU activation. Many are **"Feed-Forward Neural Networks"**, where output from previous layer is used as input for next layer.
    -  **"Deep" Networks are just NNs with more than 2 layers**. Done with gradient descent, backpropagation, and several regularization optimizers to help it run faster.
    -  We use CNNs for image processing, and RNNs for text and sequential processing.
-  Several techniques exist to combat problems of gradient vanishing/explosion, multilabel classification, nonlinear regression, and anomaly detection.
    -  ex: kernel methods for unknown dimensional data, dimensional extensions for multiclass regression, one-class algorithms for anomaly detection, etc.
    -  For the problem of semi-supervised learning, autoencoders are favorable, using encoder-decoder pairs with a hourglass shaped structure.
    -  Other experimental techniques for even faster/better learning and recognition include zero-shot and one-shot learning.
-  Usually, smaller NNs can get the job done relatively well, but if bias/variance are still too high after regularization(reduces variance), then increase the size of the NN (reduce bias)
    -  Much like normal programming, we want accurate and efficient algorithms, so we avoid unnecessary loops.
    -  Normalization is a big deal, since functions are pretty rigid (so we need to adjust the input sizes to all be one type/size).
-  Unsupervised learning is basically clustering detection, while supervised is mostly label predicting
    -  Sometimes data will be too high-dimensional, so to visualize it better we downscale the data via methods like PCA or UMAP 
## Whatâ€™s next on the list:
- Reinforcement learning and Evolutionary Algorithms, finish rest of reading list
- Calculus or discrete math
## Interesting tidbits/my thoughts:
- Interesting that the book mentioned hyperparameters, values that have to be specified outside of the environment to ensure the model isn't underfitting or overfitting.
- Matrix multiplication and vector dot products are used pretty copiously to reduce time and parameter complexity. 
    - Gradients and partial derivatives are also a key mathematical idea here, along with statistics, understandably.
- "Black boxed" algorithms do seem harder to me now, since what they really do is tune the parameters/coefficients of a function, and not much information can be drawn from just a couple of functions.
    - This probably falls under the idea of "explainable AI", where we can easily see the code to where the AI is coming from, but actually understanding what the parameters mean in our terms is pretty difficult.
    - We could probably (and already do) show the neurons that respond most strongly to an input, which could reduce some of the opacity SGD and DL algorithms naturally have.
- Zero-shot learning is most interesting to me, due to its ability to learn labels not in the dataset, such as learning what a tiger's labels are given only images of clownfish and zebras.
- Unfortunately, it seems that reinforcement learning is in a completely different boat, since it involves interactions with the environment and an expected reward function that is optimized during its interactions. 
    - Noticeably, several SL/USL algorithms use the idea of states, such as RNNs (backpropagation through time), where acting on a previous state can bring you to a new state.
- Probably many SL/USL techniques and ideas are also transferred to RL, such as gradient optimization, dimensional modifications, and autoencoders (image/text specifically)
## Data/Resources
- http://www.mlebook.com/wiki/doku.php
##### Reading List
- https://arxiv.org/abs/2105.04026
- http://cs229.stanford.edu/notes2020spring/cs229-notes1.pdf
