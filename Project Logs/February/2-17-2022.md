# Kaggle Research Code Competition 
## "TensorFlow - Help Protect the Great Barrier Reef" (Sponsored by Google)
Well, that was one hell of a competition! Link here: https://www.kaggle.com/c/tensorflow-great-barrier-reef/overview  
Solution model written, trained, and submitted on basic Colab/Kaggle environment.  

## MY RESULTS
I did WAY better than I expected! Top 6%, out of 2026 teams!
* Placed 117th out of 2026 with a final selected model submission of 0.677 on the private set (0.675 on the public set). 
    - At some point I was in the top 40, with a score of 0.628 (that later dropped down to the mid-500s as newer insights came out in the competition).
* Did little bit better on the public leaderboard, placing 112th with a public set score of 0.689 (scored 0.66 on private set).  
* My true highest submission would've hit 0.682, placing me in the top 80 teams (and therefore netting me a silver medal). It didn't do well in the public leaderboard, with a score of 0.653, so I ended up scrapping it in favor of my other public scores. 
    - Unfortunately, I didn't think too much about it, as the OOF validation scores seemed to be identical(I was using the same model, after all).  

## MY THOUGHTS:
Definitely a step up from my CAPTCHA breaker, I had to track these starfish in real time through video! Not to mention that the frames are sequential, so I have to pay attention to my validation-train splits to avoid data leakage (makes your model unusable since it can peek into the validation, which is supposed to be unaccessible). 
Upon reading the solutions of top rankers, I realized the following:
* **Many of the ideas I threw out had some strong basis to them.** 
* For example, I theorized that utilizing an ensemble with WBF smoothing could yield strong results. Although it didn't work well for me, if implemented correctly with other augmentation techniques it yielded a large jump in model accuracy and recall, increasing F2 by almost 4-5% when comparing public-private scores. 
* **Name drop of techniques I used that worked**: CV split by video/subsequences, augmentation via Albumentations (especially strong ones like rotation, blurs, and heavy downsampling), training in higher resolutions, upscaling resolutions during transfer learning, Test time augmentation, and temporal object tracking.
* Others had some weight, but ultimately overfit, causing my score to suffer. (like infer at 3,4x original resolution - ended up overfitting to public set and taking a hit on private set)
* **I needed more experience and time**: 
    - many ideas I had had to be scrapped simply because I lacked either the expertise or time to perform proper experimentation.
    - Upon reading those top solutions, I realized most of my ideas were on the right track, which was truly interesting and motivating for me.
    - **Name drop of ideas that could have been**: SAHI, CLAHE + strong albumentation during training, YOLO-Z inspired model architecture (from YOLOv5s6 architecture), inferring at original image resolution, changing the validation metric to be F2 score (got it working, but only towards the very end), WBF smoothing during inference(single model), blending in more instances of the objects of interest to increase the number of samples to train off of, and postprocessing IOU-CONF smoothing to help reduce FP rate and smoother predictions.
    - Experience wise, many of these augmentation techniques were ones I had never heard of before - it took a lot of reading and documentation to realize how they worked. 
    - Chances are, if I had more experience I would have found ways to blend more than 2 techniques together - I could only figure out how to use TTA+Tracking, and I spent a lot of time trying to get both ensembling+tracking+TTA and SAHI+tracking+TTA to work (unfortunately it didn't, which was kind of disappointing).

However, I'm not complaining - bronze on a worldwide competition, AND against people in the actual AI/ML field? I'll take that any day! 
* Besides, picking which model to be your final submission is also kind of a "skill", isn't it? 
* You want the best ones to be generalizable, after all, so that means OOF CV needs to be close to 1! 
* I wish I had a GPU (like an rtx 2060 - I would be overjoyed at a 3090), though - I had time limitations in Kaggle/Colab, and definitely could've done a smidge better, landing in the definitive silver range if I didn't have those restraints.  

## In conclusion
* Very nice competition, did better than I expected, saw the gap between first place and me, and saw both promising results as well as possible roadmaps for improvement.  
