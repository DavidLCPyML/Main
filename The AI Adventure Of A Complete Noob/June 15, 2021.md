# Overview of today: 
## What I did:  
- Expanded current DLL project to include summarizing and plotting model effectiveness
- Successfully loaded and summarized models with image visualizations
- Plotted learning curves using the fit() function in Keras
- Split original dataset into training and testing sets
- Performed a large mashup of each extension of the original project(load, summarize, split, & plot)
## What I now know:
- always keep your h5 files in different places! h5s created via saving to JSON/YAML don't have full architecture! 
- How to properly summarize model architecture via the plot_model() function
    - Parameters: 
    - model: The model to plot (required) 
    - to_file: Where to save the plot. (required) 
    - show_shapes: Whether or not to show the output shapes of each layer. (optional, defaults to False)
    - show_layer_names: Whether or not to show the name for each layer. (optional, defaults to True)
- Summary() provides the following:
    - The layers and their order in the model.
    - The output shape of each layer.
    - The number of parameters (weights) in each layer.
    - The total number of parameters (weights) in the model.
- saving to JSON or YAML only saves the architecture of the model.
- When building DNNs, make sure to 1) Confirm layer order, 2) Confirm the output shape of each layer, and 3) Confirm parameters.
    - make sure the NN is connected correctly, make sure the NN input shape is correct, and make sure the # of parameters is not too high.
- How to utilize the **History callback function** to return metrics for each of the model's 150 epochs.
    - Plots provide **convergence speed of epochs (slope)**, **the model's plateau** (whether it has already converged), or the probability of over-learning (validation line inflection).
- Doing a test and training split causes the model to take a **big hit in accuracy if the set is small**(<1000) (went from 7.5/10 to about 5.5/10 even on 90-10 splits)
    - use **train_test_split()**, which is from SciKit, to split the set. Common splits are **67-33, 80-20, and 50-50.**
    - use the **test_size** and **training_size** parameters to specify the split, and **random_state** to randomly split test and train.
    - For classification problems, use the **statify parameter** to ensure the trainng-test split contains equal % of each classification. 
    - If you have insufficient data, then use the k-fold cross-validation procedure as an alternate model evaluation procedure.
## Whatâ€™s next on the list:
- Learn more about the "Loss" value and possible optimizations
- continue reading up on the sources below
- Enact any bugfixes or improvements
- Continue extending the project:
    - Tune the Model: Change the configuration of the model or training process and see if the performance improves, e.g. better than 76% accuracy.
    - Learn a New Dataset: use a different tabular dataset, perhaps from the UCI Machine Learning Repository.
## Interesting tidbits/ my thoughts:
- So we want the loss curve to be as small as possible while also making the accuracy curve as high as possible
    - yet we ALSO want to limit the amount of space/parameters taken up by the model AND prevent overtraining and overfitting, so it's like a trade-off triangle
    - seems a lot like the privacy-convenience tradeoffs on Internet communication.
- When added to the Internet, we have a variety of tradeoffs we need to keep in mind. 
    - Especially with ideas such as BCI (brain-computer interface) circulating, we'll need to ensure maximum security as well as a set of fundamental ethics when it comes to this.
    - That being said, it's still a while off, so we can take this time to really be diligent about out research and exploration.
- Loading previously created AI models and trainig them off the same set again but with randomized test/train partitions will lead to the model overfitting.
    - while train average increases and train loss decreases, test average decreases and test loss increases. 
    - From these observations, the first training is always the best - future iterations will require completely different data values to be worthwhile.
## Data/Resources
- https://keras.io/api/models/sequential/ -> keras Sequential documentation
- https://keras.io/api/layers/core_layers/dense/ -> keras dense layer documentation
- https://machinelearningmastery.com/how-to-configure-the-number-of-layers-and-nodes-in-a-neural-network/ -> explanation of how to find NN type and # of layers
- https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/ -> how to pick the loss function
- https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch/ -> batch vs. epoch\
- https://machinelearningmastery.com/develop-evaluate-large-deep-learning-models-keras-amazon-web-services/ -> cloud GPU ML tutorials 
- https://machinelearningmastery.com/randomness-in-machine-learning/ -> "randomness" feature of DLLs
- https://machinelearningmastery.com/evaluate-skill-deep-learning-models/ -> how to "grade" DLLs
- https://machinelearningmastery.com/how-to-make-classification-and-regression-predictions-for-deep-learning-models-in-keras/ -> how keras makes its predictions
