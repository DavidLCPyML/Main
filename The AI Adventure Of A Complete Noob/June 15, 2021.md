# Overview of today: 
## What I did:  
- Expanded current DLL project to include summarizing and plotting model effectiveness
- Successfully loaded and summarized models with image visualizations
- Plotted learning curves using the fit() function in Keras
- Split original dataset into training and testing sets
## What I now know:
- always keep your h5 files in different places! h5s created via saving to JSON/YAML don't have full architecture! 
- How to properly summarize model architecture via the plot_model() function
    - Parameters: 
    - model: The model to plot (required) 
    - to_file: Where to save the plot. (required) 
    - show_shapes: Whether or not to show the output shapes of each layer. (optional, defaults to False)
    - show_layer_names: Whether or not to show the name for each layer. (optional, defaults to True)
- Summary() provides the following:
    - The layers and their order in the model.
    - The output shape of each layer.
    - The number of parameters (weights) in each layer.
    - The total number of parameters (weights) in the model.
- saving to JSON or YAML only saves the architecture of the model.
- When building DNNs, make sure to 1) Confirm layer order, 2) Confirm the output shape of each layer, and 3) Confirm parameters.
    - make sure the NN is connected correctly, make sure the NN input shape is correct, and make sure the # of parameters is not too high.
- How to utilize the **History callback function** to return metrics for each of the model's 150 epochs.
    - Plots provide **convergence speed of epochs (slope)**, **the model's plateau** (whether it has already converged), or the probability of over-learning (validation line inflection).
- Doing a test and training split causes the model to take a **big hit in accuracy if the set is small**(<1000) (went from 7.5/10 to about 5.5/10 even on 90-10 splits)
## Whatâ€™s next on the list:
- Learn more about the "Loss" value and possible optimizations
- continue reading up on the sources below
- enact any bugfixes or improvements
- Continue extending the project:
    - Tune the Model: Change the configuration of the model or training process and see if the performance improves, e.g. better than 76% accuracy.
    - Separate Train and Test Datasets: split loaded dataset into training and testing sets (based on rows) and use one to train the model and the other set to estimate the performance of the model on new data.
    - Learn a New Dataset: use a different tabular dataset, perhaps from the UCI Machine Learning Repository.
## Interesting tidbits/ my thoughts:
- So we want the loss curve to be as small as possible while also making the accuracy curve as high as possible
    - yet we ALSO want to limit the amount of space/parameters taken up by the model AND prevent overtraining and overfitting, so it's like a trade-off triangle
    - seems a lot like the privacy-convenience tradeoffs on Internet communication.
- When added to the Internet, we have a variety of tradeoffs we need to keep in mind. 
    - Especially with ideas such as BCI (brain-computer interface) circulating, we'll need to ensure maximum security as well as a set of fundamental ethics when it comes to this.
    - That being said, it's still a while off, so we can take this time to really be diligent about out research and exploration.
## Data/Resources
- https://keras.io/api/models/sequential/ -> keras Sequential documentation
- https://keras.io/api/layers/core_layers/dense/ -> keras dense layer documentation
- https://machinelearningmastery.com/how-to-configure-the-number-of-layers-and-nodes-in-a-neural-network/ -> explanation of how to find NN type and # of layers
- https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/ -> how to pick the loss function
- https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch/ -> batch vs. epoch\
- https://machinelearningmastery.com/develop-evaluate-large-deep-learning-models-keras-amazon-web-services/ -> cloud GPU ML tutorials 
- https://machinelearningmastery.com/randomness-in-machine-learning/ -> "randomness" feature of DLLs
- https://machinelearningmastery.com/evaluate-skill-deep-learning-models/ -> how to "grade" DLLs
- https://machinelearningmastery.com/how-to-make-classification-and-regression-predictions-for-deep-learning-models-in-keras/ -> how keras makes its predictions
