# Overview of today: 
## What I did:  
- Began my first DL project! Super hyped :D
    - I'm using the "Pima Indians onset of diabetes" dataset, one of the standard sets from the UCI ML repos.
    - It describes patient medical record data for Pima Indians and whether they had an onset of diabetes within five years.
    - The tutorial I'm following picked it since it fis primarily a binary classification problem as the diabetes is either 1 or 0 **(Last column is the output)**. Additionally, all inputs are also numerical.
    - This makes it easy to use directly with neural networks that expect numerical input and output values.
    - Also, diabetes datasets are not only common but also an easy beginner project, it seems.
- The NN I am making will have ReLU for the first 2 layers, then a sigmoid on the output layer to ensure our network output is between 0 and 1
    - This will make it easy to map to either a probability of class 1 or snap to a hard classification of either class with a default threshold of 0.5.
    - **Criteria:**
          - The model expects rows of data with 8 variables (the input_dim=8 argument)
          - The first hidden layer has 12 nodes and uses the relu activation function.
          - The second hidden layer has 8 nodes and uses the relu activation function.
          - The output layer has one node and uses the sigmoid activation function.
- Successfully completed the project (woohoo!)
## What I now know:
- keras.models.Sequential: https://keras.io/api/models/sequential/ -> Allows for a stack of learning layers as well as inference and training features.
- keras.layers.dense: https://keras.io/api/layers/core_layers/dense/ -> Described on documentation as the **"typical densely-connected NN-Layer"**.
    - this means that all the neurons from the previous layer connect to the ones on the current layer, so each neuron from the current layer is connected to all neurons on the previous one. 
    - **Advantage over "classic" Convolutionals: A dense one provides learning features from all possible combinations of the features of the previous layer, while a convolutional layer relies on consistent features with a small repetitive field.**
- NumPy is used for array operations, allowing use of linear algebra, fourier transforms, and matrices.
    - **Python lists are too slow.** NumPy remedies this by keeping all momory locations in 1 spot, allowing for faster runtime and use of arrays in DS and DL/ML/AI (I think).
    - NumPy is a mix of both C/C++ and Python. **The classic Data Structure is called "ndarray".**
- NNs slowly add more layers until the creator is satisfied with the architecture.
    - The first/input layer **must** have the right amount of parameters. So, in my dataset I will need 8 points for the first layer (8 columns) 
    - ReLU works better than Sigmoid and Tanh functions
- Compiling the model uses the efficient numerical libraries under the covers (the so-called backend) such as Theano or TensorFlow. 
    - The backend automatically chooses the best way to represent the network for training and making predictions to run on your hardware, such as CPU, GPU, or even distributed.
    - When compiling, additional properties need to be specified when training the network. 
          - (Training a network means finding the best set of weights to map inputs to outputs in our dataset.)
    - **The loss function evaluates a set of weights, and the optimizer searches through different weights for the network and any additional metrics we would like to collect and report during training.**
    - In this case, we will use cross entropy as the loss argument. This loss is for a binary classification problems and is defined in Keras as “binary_crossentropy“.
    - Optimizers have to be efficient and stochastic, and the gradient descent algorithm "Adam" works well here
          - Adam can automatically tune itself and give good results in a wide range of sets.
- ML Training often occurs over epochs. Each epoch, which represents **one pass through all of the rows in the training dataset**, is split into batches, which are one or more samples considered by the model within an epoch before weights are updated.
    - epochs & batches are chosen thru trial and error. 
    - The ideal trained model is able to create a good enough mapping of rows of input data to the output classification while also **sufficiently min-maxing** error. Models will always have some error, but the amount of error will level out after some point for a given model configuration (**"model convergence"**)
## What’s next on the list:
- Learn more about the "Loss" value and possible optimizations
- continue reading up on the sources below
- enact any bugfixes or improvements
- Try any of the following extensions:
        - Tune the Model: Change the configuration of the model or training process and see if the performance improves, e.g. better than 76% accuracy.
        - Save the Model: save the model to a file, then load it later and use it to make predictions (https://machinelearningmastery.com/save-load-keras-deep-learning-models/).
        - Summarize the Model: Summarize the model and create a plot of model layers (https://machinelearningmastery.com/visualize-deep-learning-neural-network-model-keras/).
        - Separate Train and Test Datasets: split loaded dataset into training and testing sets (based on rows) and use one to train the model and the other set to estimate the performance of the model on new data.
        - Plot Learning Curves: The fit() function returns a history object that summarizes the loss and accuracy at the end of each epoch. Create line plots of this data, called learning curves (https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/).
        - Learn a New Dataset: use a different tabular dataset, perhaps from the UCI Machine Learning Repository.
## Interesting tidbits/thoughts:
- I got the answer to my question yesterday: ReLU stands for **"rectified linear unit activation function"**
- Loss is preferred to be 0, and accuracy preferred as 100, but in reality it will never happen that way. Kind of like how humans, the gold standard, sometime fail the CAPTCHA, right?
## Data/Resources
- https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv -> Diabetes dataset
    - https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.names -> details abt set
- https://keras.io/api/models/sequential/ -> keras Sequential documentation
- https://keras.io/api/layers/core_layers/dense/ -> keras dense layer documentation
- https://machinelearningmastery.com/tutorial-first-neural-network-python-keras/ -> the tutorial I followed
- https://machinelearningmastery.com/how-to-configure-the-number-of-layers-and-nodes-in-a-neural-network/ -> explanation of how to find NN type and # of layers
- https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/ -> how to pick the loss function
- https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch/ -> batch vs. epoch\
- https://machinelearningmastery.com/develop-evaluate-large-deep-learning-models-keras-amazon-web-services/ -> cloud GPU ML tutorials 
- https://machinelearningmastery.com/randomness-in-machine-learning/ -> "randomness" feature of DLLs
- https://machinelearningmastery.com/evaluate-skill-deep-learning-models/ -> how to "grade" DLLs
- https://machinelearningmastery.com/how-to-make-classification-and-regression-predictions-for-deep-learning-models-in-keras/ -> how keras makes its predictions
