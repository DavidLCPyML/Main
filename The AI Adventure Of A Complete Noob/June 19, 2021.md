# Overview of today: 
## What I did:  
- Read, read, read -> I need more knowledge!
- There was an attempt to understand stock prediction -> it did not fare well :( 
## What I now know:
- ML models are essentially random -> they're stochastic in almost every step and so require the uncertainty to be minimized
    - This is where we have "bias" in some ML models because **the data is either cherry-picked, not representative enough, or the modeler is including bias**.
    - We have randomness in **Data Collection** (model variance), where the model changes based on its dataset. Remedied with more inclusive or broader dataset.
    - We have randomness in **Observation Order**, where the model may bias itself on order of observations. Remedied with random shuffling of observations before training.
    - We have randomness in **Algorithm Creation**, where the model may make decisions, especially in a draw, based on randomness. Can't really be fixed.
    - We have randomness in **Data Sampling**, where the data is too large to process in its entirety, so we use random subsamples. Also can't really be fixed.
- **Controllable randomness should be controlled**. Always ensure the model can be reproducible **using the same code, data, and randomness seed**.
- **To figure out the proper model, size, and seed, you need to do experimentation with the values**. There is no other way.
- In a similar vein, because ML algorithms ARE stochastic/random, we need to include the uncertainty -> either by reducing or reporting it.
    - Reduce it by running various models on various randomized samples, then turning the results into a population plot. Also called **k-fold cross-validation**.
    - Report it by disclosing both the highest and lowest values as well as mean and std dev (**summary stats**) after doing cross-validation. 
    - From there, ideal models can be chosen.
- Never pick one single "best" model when doing mutiple-fold cross-validations. If absolutely needed, take the first one. If not, it's better to do an ensemble (mean of all predictions). 
##### How to grade DL models
- control model variance with k-fold Cross-validation and Train-test splits.
- Control Model stability by fixing randomized seed and repeated evaluations. Recommended # of repeats is 30-100 (for expected normal distribution)
    - Then, take the overall mean and standard error of all iterations.
- Depending on the model, various NNs may be more unstable than others.
    - a sensitivity analysis (where you evaluate the model on the same data many times but changing the seed) analyzing the **skill score mean & std dev** variance in the model iterations
    - std dev is a good indicator of whether a model is unstable or not.
    - sensitivity analysis should be done at least 30 times (limited only by resources or diminished return)
## Whatâ€™s next on the list:
- Learn more about the "Loss" value and possible optimizations
- continue reading up on the sources below
- Enact any bugfixes or improvements
- Continue extending the project:
    - Tune Model: Tweak the configuration of the model or training process and see if the performance improves, e.g. better than 76% accuracy.
    - Learn a New Dataset: use a different tabular dataset, perhaps from the UCI Machine Learning Repository.
- More reading, especially from experts like Andrew Ng (https://web.archive.org/web/20180618211933/http://cs229.stanford.edu/notes/cs229-notes1.pdf)
    - read this article: https://cs231n.github.io/optimization-1/#gd
- Proposed experiment:
    - look at the impact on estimated model skill versus the number of repeats 
    - Also do it w/ the calculation of the standard error (how much the mean estimated performance differs from the true underlying population mean)
## Interesting tidbits/my thoughts:
- I really **didn't quite grasp how deep this field goes until now**: my first project is truly "beginner" as I haven't even done any summary statistics or even variance control.
- **Stock prediction based on historical data is fairly streamlined, but real-time prediction is obscenely hard**. 
    - There will always be uncertainty in predictions, and the function to measure it minimizes at a certain parameter value, much like other parameters in NNs.
    - There are a lot of models built off basic NNs -> models like **Long-Term Short-Memory** and **Feed-forward NNs** are used for stocks. 
    - Others include GANs, which are not used in stocks but rather image generation. (example in generating deepfakes and nonexistent realist images)
- Epochs and training iterations on the same dataset are **not the same**. Epochs create different models while the analysis does not.
- Interestingly enough, running more than 100 iterations for DL is hard even for beefy computers. The solution is to save the "best" model seen so far.
## Data/Resources
- https://machinelearningmastery.com/how-to-configure-the-number-of-layers-and-nodes-in-a-neural-network/ -> explanation of how to find NN type and # of layers
- https://machinelearningmastery.com/develop-evaluate-large-deep-learning-models-keras-amazon-web-services/ -> cloud GPU ML tutorials 
- https://machinelearningmastery.com/how-to-make-classification-and-regression-predictions-for-deep-learning-models-in-keras/ -> how keras makes its predictions
