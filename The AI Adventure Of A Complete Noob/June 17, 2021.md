# Overview of today: 
## What I did:  
- Read, read, read -> I need more knowledge!
## What I now know:
- Stochastic GD, the DL approach I used for the Pima Indians set, computes errors and updates weights in faster iterations (since it only processes a small selection of samples per epoch) and reaches optimums quicker. ITs downfall is the noisy curve with fluctutaitons between each epoch.
    - This is because the mean error over the stochastically/randomly selected subset, is calculated in each iteration. 
    - Some samples will produce higher error, some lower. Since it's not the actual average, SGD mis more volatile in its curve predictions.
- There are many types of loss functions, and you need to pick the appropriate one for the analysis.
- We have linear loss algorithms:
    - Mean Squared Error Loss: The Gold Standard, **only change if you have a good reason to suspect the data is not Gaussian**. Penalizes models for bigger mistakes.
    - Mean Squared Logarithmic Error Loss: To be used if there is a large spread of values and **you need to predict large values**.
    - Mean Absolute Error: When you have a mostly Gaussian distribution with several outliers.
- Functionally, NNs work better when the real-valued input and output variables are scaled to a sensible range. 
    - i.e. the input variables and the target variable have a Gaussian distribution, so standardizing the data is desirable.
## Whatâ€™s next on the list:
- Learn more about the "Loss" value and possible optimizations
- continue reading up on the sources below
- Enact any bugfixes or improvements
- Continue extending the project:
    - Tune the Model: Change the configuration of the model or training process and see if the performance improves, e.g. better than 76% accuracy.
    - Learn a New Dataset: use a different tabular dataset, perhaps from the UCI Machine Learning Repository.
- More reading, especially from experts like Andrew Ng (https://web.archive.org/web/20180618211933/http://cs229.stanford.edu/notes/cs229-notes1.pdf)
    - read this article: https://cs231n.github.io/optimization-1/#gd
## Interesting tidbits/ my thoughts:
- 
## Data/Resources
- https://machinelearningmastery.com/how-to-configure-the-number-of-layers-and-nodes-in-a-neural-network/ -> explanation of how to find NN type and # of layers
- https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/ -> how to pick the loss function
- https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch/ -> batch vs. epoch\
- https://machinelearningmastery.com/develop-evaluate-large-deep-learning-models-keras-amazon-web-services/ -> cloud GPU ML tutorials 
- https://machinelearningmastery.com/randomness-in-machine-learning/ -> "randomness" feature of DLLs
- https://machinelearningmastery.com/evaluate-skill-deep-learning-models/ -> how to "grade" DLLs
- https://machinelearningmastery.com/how-to-make-classification-and-regression-predictions-for-deep-learning-models-in-keras/ -> how keras makes its predictions
