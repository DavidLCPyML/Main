# Overview of today: 
## What I did:  
- Read, read, read -> I need more knowledge!
## What I now know:
- Stochastic GD, the DL approach I used for the Pima Indians set, computes errors and updates weights in faster iterations (since it only processes a small selection of samples per epoch) and reaches optimums quicker. Its downfall is the noisy curve with fluctutaitons between each epoch.
    - Classic GD minimizes the _gradient_, also called "error". 
    - This is because the mean error over the stochastically/randomly selected subset, is calculated in each iteration. 
    - Some samples will produce higher error, some lower. Since it's not the actual average, SGD mis more volatile in its curve predictions.
- There are many types of loss functions, and you need to pick the appropriate one for the analysis.
- Functionally, NNs work better when the real-valued input and output variables are scaled to a sensible range. 
    - i.e. the input variables and the target variable have a Gaussian distribution, so standardizing the data is desirable.
###### **linear loss algorithms:** Predict a real-value quantity, i.e. regression
- **Mean Squared Error Loss:** The Gold Standard, **only change if you have a good reason to suspect the data is not Gaussian**. Penalizes models for bigger mistakes. Requires "linear" activation in output layer.
- **Mean Squared Logarithmic Error Loss**: To be used if there is a large spread of values and **you need to predict large values**.
- **Mean Absolute Error**: When you have a mostly **Gaussian distribution with several outliers**.
###### **Binary Classification Loss:** One class to predict, a yes/no type of question
- **Cross-Entropy: Gold Standard** for classification. Calculates a score that summarizes the average difference between the actual and predicted probability distributions for predicting a "1". Ideal score is 0. Requires "sigmoid" activation in output layer.
- **Hinge Loss**: Used in Support Vector Machine (SVM), **intended for values in range {-1,1}**. Punishes model for having different signs. Sometimes better than cross-entropy.
- **Squared Hinge Loss**: Simply squares the hinge loss. Makes the surface of the error function smoother and numerically easier to work with. Requirements similar to Hinge Loss.
###### Multi-Classification Loss: More than 2 classifications
- **Multi-Class Cross-Entropy Loss**: **Gold standard**. Score is the average difference between the actual and predicted probability distributions for all classes in the problem. **Requires target var to be hot-encoded via to_categorical()**, as well as n output nodes w/ "softmax" activation in output layer.
- **Sparse Multi-Class Cross-Entropy Loss**: When there are way **too many labels** to efficiently encode. Used for vocab prediction. Identical to MCCE minus the hot encoding requirement. 
- **Kullback-Leibler Divergence Loss AKA KL Divergence**: Similar to Cross-Entropy. Measures how much a probability distributions differs from baseline (**0 is ideal**). Commonly used to **approximate a more complex function** than simply multi-class classification. Can be used in multiclass classifying though, similar to MCCE.
###### Batch vs Epoch
- Batch: the number of samples to go thru before updating parameters. Updates parameters at end of batch.
    - A training set can be split into multiple batches.
    - Batch GD: Batch Size = Training set size
    - Stochastic GD: Batch size = 1
    - Mini-Batch GD: 1 < Batch Size < Training Set Size (notably Batch Size = 32, 64, and 128)
    - If uneven batch size, you can decrease samples until you get a clean division.
- Epoch: number of times the algorithm goes thru the training set. Traditionally large (>100). Used as x-axis for learning curve plots.
    - Total batches = Epochs * batches
## Whatâ€™s next on the list:
- Learn more about the "Loss" value and possible optimizations
- continue reading up on the sources below
- Enact any bugfixes or improvements
- Continue extending the project:
    - Tune Model: Tweak the configuration of the model or training process and see if the performance improves, e.g. better than 76% accuracy.
    - Learn a New Dataset: use a different tabular dataset, perhaps from the UCI Machine Learning Repository.
- More reading, especially from experts like Andrew Ng (https://web.archive.org/web/20180618211933/http://cs229.stanford.edu/notes/cs229-notes1.pdf)
    - read this article: https://cs231n.github.io/optimization-1/#gd
## Interesting tidbits/my thoughts:
- That bit about GD was surprising, I didn't expect it to be merely an optimization algorithm for NNs to train off of.
- Seems like NLP may use the sparse version of MCCE. Could that also be used in the text sentence predictions? Would that take up too much power?
## Data/Resources
- https://machinelearningmastery.com/how-to-configure-the-number-of-layers-and-nodes-in-a-neural-network/ -> explanation of how to find NN type and # of layers
- https://machinelearningmastery.com/develop-evaluate-large-deep-learning-models-keras-amazon-web-services/ -> cloud GPU ML tutorials 
- https://machinelearningmastery.com/randomness-in-machine-learning/ -> "randomness" feature of DLLs
- https://machinelearningmastery.com/evaluate-skill-deep-learning-models/ -> how to "grade" DLLs
- https://machinelearningmastery.com/how-to-make-classification-and-regression-predictions-for-deep-learning-models-in-keras/ -> how keras makes its predictions
