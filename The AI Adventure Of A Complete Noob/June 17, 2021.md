# Overview of today: 
## What I did:  
- Read, read, read -> I need more knowledge!
## What I now know:
- Stochastic GD, the DL approach I used for the Pima Indians set, computes errors and updates weights in faster iterations (since it only processes a small selection of samples per epoch) and reaches optimums quicker. Its downfall is the noisy curve with fluctutaitons between each epoch.
    - Classic GD minimizes the _gradient_, also called "error". 
    - This is because the mean error over the stochastically/randomly selected subset, is calculated in each iteration. 
    - Some samples will produce higher error, some lower. Since it's not the actual average, SGD mis more volatile in its curve predictions.
- There are many types of loss functions, and you need to pick the appropriate one for the analysis.
####### **linear loss algorithms:** Predict a real-value quantity, i.e. regression
    - **Mean Squared Error Loss:** The Gold Standard, **only change if you have a good reason to suspect the data is not Gaussian**. Penalizes models for bigger mistakes. Requires "linear" activation in output layer.
    - **Mean Squared Logarithmic Error Loss**: To be used if there is a large spread of values and **you need to predict large values**.
    - **Mean Absolute Error**: When you have a mostly **Gaussian distribution with several outliers**.
- **Binary Classification Loss:** One class to predict, a yes/no type of question
    - **Cross-Entropy: Gold Standard** for classification. Calculates a score that summarizes the average difference between the actual and predicted probability distributions for predicting a "1". Ideal score is 0. Requires "sigmoid" activation in output layer.
    - **Hinge Loss**: Used in Support Vector Machine (SVM), **intended for values in range {-1,1}**. Punishes model for having different signs. Sometimes better than cross-entropy.
    - **Squared Hinge Loss**: Simply squares the hinge loss. Makes the surface of the error function smoother and numerically easier to work with. Requirements similar to Hinge Loss.
###### Multi-Classification Loss: More than 2 classifications
    - **Multi-Class Cross-Entropy Loss**: **Gold standard**. Score is the average difference between the actual and predicted probability distributions for all classes in the problem. **Requires target var to be hot-encoded via to_categorical()**, as well as n output nodes w/ "softmax" activation in output layer.
    - **Sparse Multi-Class Cross-Entropy Loss**: When there are way **too many labels** to efficiently encode. Used for vocab prediction/NLP. Identical to MCCE minus the hot encoding requirement. 
    - **Kullback Leibler Divergence Loss AKA KL Divergence**: Similar to Cross-Entropy. Measures how much a probability distributions differs from baseline (**0 is ideal**). Commonly used to **approximate a more complex function** than simply multi-class classification. Can be used in multiclass classifying though, similar to MCCE.
####### Batch vs Epoch
- Functionally, NNs work better when the real-valued input and output variables are scaled to a sensible range. 
    - i.e. the input variables and the target variable have a Gaussian distribution, so standardizing the data is desirable.
## Whatâ€™s next on the list:
- Learn more about the "Loss" value and possible optimizations
- continue reading up on the sources below
- Enact any bugfixes or improvements
- Continue extending the project:
    - Tune Model: Tweak the configuration of the model or training process and see if the performance improves, e.g. better than 76% accuracy.
    - Learn a New Dataset: use a different tabular dataset, perhaps from the UCI Machine Learning Repository.
- More reading, especially from experts like Andrew Ng (https://web.archive.org/web/20180618211933/http://cs229.stanford.edu/notes/cs229-notes1.pdf)
    - read this article: https://cs231n.github.io/optimization-1/#gd
## Interesting tidbits/ my thoughts:
- 
## Data/Resources
- https://machinelearningmastery.com/how-to-configure-the-number-of-layers-and-nodes-in-a-neural-network/ -> explanation of how to find NN type and # of layers
- https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch/ -> batch vs. epoch\
- https://machinelearningmastery.com/develop-evaluate-large-deep-learning-models-keras-amazon-web-services/ -> cloud GPU ML tutorials 
- https://machinelearningmastery.com/randomness-in-machine-learning/ -> "randomness" feature of DLLs
- https://machinelearningmastery.com/evaluate-skill-deep-learning-models/ -> how to "grade" DLLs
- https://machinelearningmastery.com/how-to-make-classification-and-regression-predictions-for-deep-learning-models-in-keras/ -> how keras makes its predictions
