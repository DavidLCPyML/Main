# Overview of today: 
## What I knew beforehand:  
- 
## What I did:  
- Attended talk by UT prof. Adam klivans about Algorithmic Challenges in Deep Learning 
- Continued familiarizing myself with the technical knowledge on the websites found on Thursday/Friday
## What I now know:
- There is a large gap btwn theory and practice (expected though).
- New ML algorithms such as new insights on NNs or improved reasoning may lead to huge advances.
- Linear Regression is the primary example of ML's prediction applications, since you predict values in a regression thingy.
    - when it comes to predictions, you will need assumptions so you don't end up getting completely trivial outliers
    - additionally, proving that it is 100% accurate and that it accounts for all outliers is a hard and nontrivial task.
- Classification is a similar task: find a line __L__ where all +1s are on one side of __L__ and all -1s are on the other side
    - its uglier counterpart, multidimensional classification, may require linear programming & other nasty stuff I won't be able to do now.
    - Furthermore, there are an infinite number of these separators, but we need to find the best generalizer.
          -  it's possible to prove, though, that with D dimensions and O(D) points you can have any separator L will have predictive power.
          -  this is because we split the data, allowing us to generalize.
- Conventional wisdom in AI states that **few, less complicated, or "small" models implies predictive power**. (similar vein to __"overtraining leads to disaster"__).
    - lack of compression = no predictive power
    - more complex models are typically punished, except for more complex ideas like:
- **Modern NNs have more parameters than the size of the training set**.
    -  Examples include the STYLEGAN2 network.
    -  This is currently unsolved; no one knows why this is the case.
    -  Speculation includes: 1) Implicit bias exists in the algorithm, 2) There is hidden structures in the dataset the models are taking advantage of
    -  People have also proved that these NN algorithms will never converge (backpropagation, gradient descent)
## Whatâ€™s next on the list:
- Explore Alex Dimakis's paper on ArXiv & explore it
- comtinue reading & acquiring extra technical knowledge on AI experiments
- brush up on Calculus, linear Algebra, and stats for extra prep (maybe even a bit of real/complex analysis)
- IFML, Machine Learning Lab, etc.
## Interesting tidbits/thoughts:
- Interesting that PULSE, the previous deepfake face generation Algorithm, tended to create faces on the more Caucasian/white side (Obama and Samuel L. Jackson deepfakes look more white than the current STYLEGAN2 algorithm). Could that be due to an overabundance of white/caucasian faces in the dataset?
- Might actually pursue research at the ML Laboratory at UT...
- Could this be related to the "curse of dimensionality" idea (more data = more complicated stuff w/ higher dimensions)?
- Could better algorithms or alternatives to GD/Backpropagation fix or remedy this phemonema?
- What is ReLU? seems to be pretty famous in DL professionals
