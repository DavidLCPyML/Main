# Overview: 
## What I did (over the past week or so):  
- Built a (several) model(s) that attempted to predict the prices of a stock based on past values. (LSTM, DTree, and ARIMA)
- Tested those models against real data from Alpha Vantage.
## What I now know:
-  Proving a model can reliably predict the stock market is nontrivial, since there are too many variables the model isn't structured to take into account. 
    -  No matter how many neurons you add, there will always be that one catastrophic edge scenario that throws off the model completely.
-  Technical, hands-on coding knowledge: 
    -  How to split training sets, how to scale data, how to predict future values, and most importantly how to get data off the web.
-  In a nutshell, traditional programming infers answers from data and rules, but machine learning infers rules from data and answers. 
-  CNNs are very good at extracting features from images
## Whatâ€™s next on the list:
- Continue learning and gathering fundamentals through literature
- Brush up on calculus and linear algebra, maybe?
## Interesting tidbits/my thoughts:
- Conclusion about the project: https://github.com/flubsmeny/flubsmeny/blob/main/AI%20Adventure%20-%20Projects/Stock%20Prediction/Conclusion%20%26%20sources.md
    - Should be taken with a grain of salt, I am inexperienced so this is not truly representative of the state of AI
- Could reward modelling work better? If a human put their feedback in a training model as seen with the agents in https://arxiv.org/abs/1811.07871, could prediction in the sense of accurate, precise putputs be more feasible?
    - Conversely, would that lead to a worse outcome or even a new optimizer having to be written?
- This was one hell of a grueling project, though it was not only good practice, but also a nice reality check about the limits of AI as it is now.
- No one tells a child to see or interpret, they just jump directly to "testing". Computers are still in the step of being in "training".
- relu is basically a better version of sigmoid as it allows for a "ramp" structure over the inverse tangent shape of the sigmoid function, which enables better training of deep networks
## Data/Resources
- https://github.com/flubsmeny/flubsmeny/blob/main/AI%20Adventure%20-%20Projects/Stock%20Prediction/Conclusion%20%26%20sources.md
##### Reading List
- http://www.mlebook.com/wiki/doku.php
- https://arxiv.org/abs/2105.04026
- http://cs229.stanford.edu/notes2020spring/cs229-notes1.pdf
