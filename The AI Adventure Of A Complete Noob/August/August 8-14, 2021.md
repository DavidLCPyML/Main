# Overview: 
## What I did:  
- Looked at the MNIST dataset, attempting to apply simple Kmeans and CNN models to predict the digit written in the input image
  - Unsurprisingly, the CNN did much better than the Kmeans algorithm at predicting the drawn image
- tried running an RL project that attempted to play Atari breakout
  - failed (MLE on first iteration) due to the insanely large memory and processing power required, may return later when I get access to better resources
## What I now know:
- Kmeans, while a solid algorithm, does not work very well with the MNIST set in particular due to being an unsupervised algorithm.
  - **Supervised will always outperform unsupervised on labeled datasets**.
  - Another argument as to why Kmeans and MNIST don't mix well is due to the fact that the output is basically categorical (one-hot encoded 0-9 multiclass), so running it resulted in gibberish values.
- Noticeably, CNNs work better than Kmeans not only because MNIST is SL but also because CNNs can perform convolutions, which highlight certain features depending on the filter.
  - The CNN performed very well, only getting a small percentage of the test data wrong. It worked very well on images that were not in the dataset, too, only getting '9' wrong.
- It's **always good to use validation sets** on large datasets to know whether your baseline model will work well or not.
- **ReLU is almost always used as non-output activation in NNs**. Outputs are usually softmax or sigmoid.
  - The He_uniform initializer is also generally used, due to its stability, speed, and ease in determining accuracy convergence.
  - Pooling and Flattening is also useful (and sometimes necessary) to reduce dimensions, highlight features, and decrease computations needed.
- When resizing images, the images may lose some features
  - for example, lines may be more faded than usual, some curves may not come out correctly (half-completed, fuzzy)
- Images are considered very high-dimensional, so resizing them to a vector requires reshaping and transforming them.
  - For example, a full-color n x m image has shape (n, m, 3) due to RGB and the pixel dimensions; transforming into a feature vector will require some preprocessing.
## Whatâ€™s next on the list:
- Apply CNN to other images, like with animals or cars.
  - Look at higher-level techniques like GANs, maybe?
## Interesting tidbits/my thoughts:
- CNNs are very good at what they do (image/text recognition), and it doesn't take too long to train them, making sense that they'd be used for computer vision
- Honestly expected kmeans to perform better than it did on the test image - articles show that it can reach >95% accuracy
  - Possible explanations: misinterpreting the outputs or not visualizing correctly.
## Data/Resources
- https://machinelearningmastery.com/how-to-develop-a-convolutional-neural-network-from-scratch-for-mnist-handwritten-digit-classification/ - the tutorial used
- http://yann.lecun.com/exdb/mnist/ - the dataset (included in keras already)
- https://towardsdatascience.com/weight-initialization-for-neural-networks-does-it-matter-e2fd99b3e91f - CNN weight initializer comparisons
##### Reading List
- https://arxiv.org/abs/2105.04026
- http://cs229.stanford.edu/notes2020spring/cs229-notes1.pdf
